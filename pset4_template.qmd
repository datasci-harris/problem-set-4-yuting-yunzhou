---
title: "Your Title"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 
    
## Style Points (10 pts) 
Please refer to the minilesson on code style
**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 

```{python}
import geopandas as gpd
```

## Download and explore the Provider of Services (POS) file (10 pts)

1. 

```{python}
import pandas as pd
pos2016 = pd.read_csv("POS_File_Hospital_Non_Hospital_Facilities_Q4_2016.csv")
print (pos2016.head())
print(pos2016.columns)
```

The variables that I pulled are provider type code, provider subtype code, city name, facility name, provider number, state code, zip code.

# Count the number of short-term hospitals

2. 
    a.
    ```{python}
    import pandas as pd
    short_term_hospitals = pos2016[(pos2016['PRVDR_CTGRY_CD'] == 1) & (pos2016['PRVDR_CTGRY_SBTYP_CD'] == 1)]
    num_hospitals = len(short_term_hospitals)
    print(f"Number of short-term hospitals in 2016: {num_hospitals}")

    ```

    b.I used the American Hospital Association (AHA), which provides comprehensive statistics on U.S. hospitals. According to the AHA's "Fast Facts on U.S. Hospitals" for 2018, there were 5,534 registered hospitals in the U.S. in 2016.

    link: https://www.aha.org/system/files/2018-02/2018-aha-hospital-fast-facts.pdf

    
3. 

```{python}
file_names = [
    "POS_File_Hospital_Non_Hospital_Facilities_Q4_2016.csv",
    "POS_File_Hospital_Non_Hospital_Facilities_Q4_2017.csv",
    "POS_File_Hospital_Non_Hospital_Facilities_Q4_2018.csv",
    "POS_File_Hospital_Non_Hospital_Facilities_Q4_2019.csv"
]
years = [2016, 2017, 2018, 2019]
pos_data = []

for year, file in zip(years, file_names):
    try:
        pos_year = pd.read_csv(file, encoding='ISO-8859-1')
        
        short_term = pos_year[(pos_year['PRVDR_CTGRY_CD'] == 1) & 
                              (pos_year['PRVDR_CTGRY_SBTYP_CD'] == 1)]
        
        short_term['Year'] = year
        
        pos_data.append(short_term)
    
    except UnicodeDecodeError as e:
        print(f"Could not read {file} due to encoding issues: {e}")

pos_all = pd.concat(pos_data, ignore_index=True)
print(pos_all.head())
```

```{python}
import altair as alt

hospital_counts_by_year = pos_all.groupby('Year').size().reset_index(name='Count')

chart = alt.Chart(hospital_counts_by_year).mark_bar(size=15).encode(
    x=alt.X('Year:O', title='Year'), 
    y=alt.Y('Count:Q', title='Number of Observations'), 
    tooltip=['Year', 'Count']  
).properties(
    width=170,
    title="Number of Short-Term Hospitals by Year (2016–2019)"
)

text = chart.mark_text(
    align='center',
    baseline='bottom',
    dy=-5  
).encode(
    text='Count:Q'
)
  
chart+text
```

4. 
    a.
    b.

## Identify hospital closures in POS file (15 pts) (*)

1.  
```{python}
import pandas as pd

pos2016 = pd.read_csv("POS_File_Hospital_Non_Hospital_Facilities_Q4_2016.csv", encoding="ISO-8859-1")
pos2017 = pd.read_csv("POS_File_Hospital_Non_Hospital_Facilities_Q4_2017.csv", encoding="ISO-8859-1")
pos2018 = pd.read_csv("POS_File_Hospital_Non_Hospital_Facilities_Q4_2018.csv", encoding="ISO-8859-1")
pos2019 = pd.read_csv("POS_File_Hospital_Non_Hospital_Facilities_Q4_2019.csv", encoding="ISO-8859-1")

print("Columns in 2016 data:", pos2016.columns.tolist())

active_2016 = pos2016[['FAC_NAME', 'ZIP_CD', 'PRVDR_NUM']].copy()

def find_closures(active_df, *args):
    closures = active_df.copy()
    closures['Suspected_Closure_Year'] = None

    for year, df in enumerate(args, start=2017):
        closures.loc[~closures['PRVDR_NUM'].isin(df['PRVDR_NUM']), 'Suspected_Closure_Year'] = year

    return closures.dropna(subset=['Suspected_Closure_Year'])

suspected_closures = find_closures(active_2016, pos2017, pos2018, pos2019)

print("Total suspected closures:", len(suspected_closures))
print(suspected_closures.sort_values('FAC_NAME').head(10))
```
There are 3 hospitals that fir this definition.

2.   
```{python}
print(suspected_closures.sort_values('FAC_NAME')[['FAC_NAME', 'Suspected_Closure_Year']].head(10))
```

3.  
```{python}
zip_counts_2016 = pos2016.groupby('ZIP_CD').size()
zip_counts_2017 = pos2017.groupby('ZIP_CD').size()
zip_counts_2018 = pos2018.groupby('ZIP_CD').size()
zip_counts_2019 = pos2019.groupby('ZIP_CD').size()

suspected_closures['is_merger'] = suspected_closures.apply(
    lambda row: (
        zip_counts_2016.get(row['ZIP_CD'], 0) == zip_counts_2017.get(row['ZIP_CD'], 0) or
        zip_counts_2017.get(row['ZIP_CD'], 0) == zip_counts_2018.get(row['ZIP_CD'], 0) or
        zip_counts_2018.get(row['ZIP_CD'], 0) == zip_counts_2019.get(row['ZIP_CD'], 0)
    ), axis=1
)

filtered_closures = suspected_closures[~suspected_closures['is_merger']]

print("Total closures after filtering mergers:", len(filtered_closures))
print(filtered_closures.sort_values('FAC_NAME')[['FAC_NAME', 'ZIP_CD', 'Suspected_Closure_Year']].head(10))
```
    a. Among the suspected closures, how many hospitals fit this definition of potentially being a merger/acquisition?
    b. After filtering out these potential mergers or acquisitions, 0 hospitals remain in the list of closures.
    c. Since there are no remaining hospitals after filtering, the sorted list of corrected hospital closures is empty.

## Download Census zip code shapefile (10 pt) 

1. 
    a.
    b. 
2. 

## Calculate zip code’s distance to the nearest hospital (20 pts) (*)

1.  
```{python}
import geopandas as gpd

zip_shapefile = gpd.read_file("gz_2010_us_860_00_500k.shp")

zips_all_centroids = zip_shapefile.copy()
zips_all_centroids['centroid'] = zips_all_centroids.geometry.centroid

print("Dimensions of zips_all_centroids:", zips_all_centroids.shape)
print("Columns in zips_all_centroids:", zips_all_centroids.columns.tolist())

```

2.   
```{python}
zips_texas_centroids = zips_all_centroids[zips_all_centroids['ZIP_CODE'].str.startswith(('75', '76'))]

bordering_state_prefixes = ('75', '76', '77', '78')  
zips_texas_borderstates_centroids = zips_all_centroids[zips_all_centroids['ZIP_CODE'].str.startswith(bordering_state_prefixes)]

print("Number of Texas zip codes:", len(zips_texas_centroids))
print("Number of Texas and border states zip codes:", len(zips_texas_borderstates_centroids))

```

3.  
```{python}
hospitals_2016 = pos2016[['ZIP_CODE']].drop_duplicates()
zips_withhospital_centroids = zips_texas_borderstates_centroids.merge(
    hospitals_2016, on='ZIP_CODE', how='inner'
)

print("Number of zip codes with at least one hospital:", len(zips_withhospital_centroids))
```

4.   
```{python}
from shapely.ops import nearest_points

def calculate_nearest_hospital(zip_df, hospital_df):
    distances = []
    for zip_centroid in zip_df['centroid']:
        nearest_hospital = nearest_points(zip_centroid, hospital_df.unary_union)[1]
        distance = zip_centroid.distance(nearest_hospital)
        distances.append(distance)
    return distances

subset_distances = calculate_nearest_hospital(zips_texas_centroids.head(10), zips_withhospital_centroids)
print("Distance for 10 zip codes:", subset_distances)

zips_texas_centroids['distance_to_nearest_hospital'] = calculate_nearest_hospital(zips_texas_centroids, zips_withhospital_centroids)

```
    a.
    b.
5.   
```{python}
zips_texas_centroids['distance_to_nearest_hospital_miles'] = zips_texas_centroids['distance_to_nearest_hospital'] * 0.000621371

average_distance = zips_texas_centroids['distance_to_nearest_hospital_miles'].mean()
print(f"Average distance to nearest hospital (in miles): {average_distance}")

import matplotlib.pyplot as plt

zips_texas_centroids.plot(column='distance_to_nearest_hospital_miles', cmap='coolwarm', legend=True)
plt.title("Distance to Nearest Hospital by Zip Code in Texas")
plt.show()

```

    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts)

1. 
2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 
